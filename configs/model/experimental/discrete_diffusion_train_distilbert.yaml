_target_: src.experimental.modules.discrete_diffusion_module_distilbert.DiscreteDiffusionModule

codebooks: ${data_loading.train_dataloader_config.dataloader.dataset_config.semantic_id_map.sequence_data}
num_hierarchies: 4
data_freqs_path: ${data_freqs_path} # Path to the data frequencies file

vocab_size: 256     
embedding_dim: ${embedding_dim} 
use_token_type_embedding: ${use_token_type_embedding} # Whether to use token type embedding

masking_token_id: ${model.vocab_size}  # Masking token ID, should be the last ID in the vocabulary
padding_token_id: -1

# Positional embedding model
positional_embedding:
  _target_: torch.nn.Embedding
  num_embeddings: 128  # Maximum sequence length (adjust as needed)
  embedding_dim: ${embedding_dim} 

# token_type_embedding:
#   _target_: torch.nn.Embedding
#   num_embeddings: ${model.num_hierarchies}    # Two types: item and position
#   embedding_dim: ${embedding_dim}

# model:
#   _target_: torch.nn.TransformerEncoder
#   encoder_layer:
#     # _target_: src.experimental.modules.custom_transformer_encoder_layer.CustomTransformerEncoderLayer
#     _target_: torch.nn.TransformerEncoderLayer
#     d_model: ${embedding_dim}         # Should match embedding_dim
#     nhead: 8              # Number of attention heads
#     dim_feedforward: 3072 
#     # num_hidden_layers: 2  # Number of hidden layers in the feedforward network
#     dropout: 0.25
#     activation: "gelu"
#     batch_first: true
#     norm_first: true
#   num_layers: 8

model:
  _target_: transformers.models.distilbert.modeling_distilbert.Transformer
  config:
    _target_: transformers.DistilBertConfig
    n_layers: 8
    n_heads: 8
    dim: ${embedding_dim}
    hidden_dim: 3072
    activation: "gelu"


diffusion_config:
  num_steps: ${model.num_hierarchies}  ## Number of diffusion steps=num_hierarchies
  num_candidates: 10

  inference_type: "beam-search-generation" ## Options: "beam-search-generation", "constrained-beam-search-generation". 
                                 ## "beam-search-generation" means beam search generation, 
                                 ## "constrained-beam-search-generation" means constrained beam search generation.
  
  constrained_beam_refinement_steps: 10 ## Number of refinement steps for constrained beam search generation.
  constrained_beam_initial: "generated" ## Options: "random", "generated". 
                                 ## "random" means random initial candidates, "generated" means generated initial candidates.
  prune_generation_values: "multi-step" ## Options: "one-step", "multi-step". 
                                 ## "one-step" means prune generation values after one step, "multi-step" means prune generation values after multiple steps.

  loss_reweighting: "normalized"      ## Options: "equal", "normalized". "equal" means all seqs are equally weighted, 
                                 ## "normalized" means all seqs are weighted by # masked positions in the sequence.
  noise_schedule: "uniform"          ## Options: "uniform", "edm", "last-token-ar". 
                                 ## "uniform" means uniform noise schedule, "edm" means edm noise schedule, "last-token-ar" means last token autoregressive noise schedule.
  unmasking_type: "top-prob"       ## Options: "random", "top-prob", "top-prob-dup-last", "left-to-right". 
                                 ## "random" means randomly select the positions to predict, "top-prob" means predict the top probability tokens.
  unmasking_temperature: 0.01
  maskout_masking_token_prob_decoding: true ## If true, the masking token will not be predicted during decoding.

loss_function: ${loss.loss_function}

optimizer: ${optim.optimizer}

scheduler: ${optim.scheduler}

evaluator: ${eval.evaluator}