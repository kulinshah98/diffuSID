## Loads a T5 encoder model per domain and use information bottlenecks for the communication between multiple domain.


defaults:
  - hf_transformer
  - huggingface_model/t5_encoder@domain_models.16
  - huggingface_model/t5_encoder@domain_models.17
  - _self_

_target_: src.models.internal.modules.huggingface.transformer_module_cross_domain_sparse_id.CDSIDIBTransformerModule

# setting num_layers to 0 such that cross domain model does not have a shared encoder
huggingface_model:
  config:
    num_heads: 4
    num_layers: 0

domain_models:
  '16': # DISCOVER_FEED
   config:
    num_heads: 3
    num_layers: 2
    vocab_size: 0 # setting this to 0 to prevent the model from using the vocab
  '17': # SPOTLIGHT_FEED
   config:
    num_heads: 3
    num_layers: 2
    vocab_size: 0 # setting this to 0 to prevent the model from using the vocab

feature_to_model_input_map:
  sequencefeature_unified_static__creator_id__categorical_vocab: input_ids
  sequencefeature_unified_static__page_type__categorical_vocab: domain_ids

masking_token: ${data_loading.train_dataloader_config.dataloader.masking_token}
num_ib_tokens: 1
num_placeholder_ids: ${data_loading.train_dataloader_config.dataset_config.num_placeholder_tokens_map.sequencefeature_unified_static__creator_id__categorical_vocab}
ib_comm_layers:
  - !!int 0
