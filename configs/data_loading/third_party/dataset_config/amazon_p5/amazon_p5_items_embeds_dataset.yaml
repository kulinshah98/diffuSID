## This is a dataset config file for datasets of items constructed from Amazon P5 data.
dataset:
  _target_: src.experimental.data.loading.components.interfaces.ItemDatasetConfig
  item_id_field: id
  keep_item_id: True
  iterate_per_row: True
  data_iterator:
    _target_: src.data.loading.components.iterators.TFRecordIterator

  # Although the P5 dataset contains pre-computed item embeddings, we ignore them here
  # because they are not as powerful as other embeddings we can use.
  # Instead, in `embedding_map` we load a tensor of embeddings that maps from item ID to embedding.
  # We recommend only using pre-computed embeddings from the P5 dataset for debugging/quick dev purposes.
  features_to_consider: ${extract_fields_from_list_of_dicts:${data_loading.features_config.features}, "name", False, "is_item_ids", "True"}
  embedding_map: ${create_map_from_list_of_dicts:${data_loading.features_config.features}, "name", "embeddings"}
  num_placeholder_tokens_map: ${create_map_from_list_of_dicts:${data_loading.features_config.features}, "name", "num_placeholder_tokens"}

  # We need the partial after the name otherwise hydra will try to run the function when the program starts.
  # processing functions can have a features_to_apply field with a list of features to apply the function to. If not present, the function will be applied to all features.
  preprocessing_functions:
    - _target_: src.data.loading.components.pre_processing.filter_features_to_consider
      _partial_: True
    - _target_: src.data.loading.components.pre_processing.convert_to_dense_numpy_array
      _partial_: True
    - _target_: src.data.loading.components.pre_processing.convert_fields_to_tensors
      _partial_: True
    - _target_: src.experimental.data.loading.components.pre_processing.map_sparse_id_to_embedding
      _partial_: True
      sparse_id_field: id
      embedding_field_to_add: embedding
  field_type_map: ${create_map_from_list_of_dicts:${data_loading.features_config.features}, "name", "type"}
