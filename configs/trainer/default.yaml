defaults:
  - _self_
  - profiler: none
_target_: lightning.pytorch.trainer.Trainer

default_root_dir: ${paths.output_dir}
min_steps: 1
# remember that if you use gradient accumulation, this number will only be updated after each accumulation
# so for 40000 steps with gradient accumulation of 2, you will have 80000 batches
max_steps: !!int 80000
max_epochs: 10
accelerator: cpu
devices: 1
num_nodes: 1
# mixed precision for extra speed-up
precision: bf16-mixed
log_every_n_steps: 2500
# perform a validation loop every N training epochs
val_check_interval: 5000

# set True to to ensure deterministic results
# makes training slower but gives more reproducibility than just setting seeds
deterministic: False
accumulate_grad_batches: 1
