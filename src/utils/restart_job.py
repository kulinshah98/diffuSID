import os
import signal
import subprocess
import sys
import tempfile
import time
import traceback
from datetime import datetime
from typing import Any, Callable, TypeVar, Union
from lightning import Trainer
from lightning.pytorch.callbacks import Callback
from lightning.pytorch.utilities.rank_zero import rank_zero_only
from omegaconf import DictConfig
from src.utils.pylogger import RankedLogger
from src.utils.restart_job_utils import (
    JobCheckpointMetadata,
    RestartMetadata,
    _copy_time_dependent_args,
    _is_process_running,
    clean_up_resources,
    get_attribute_from_metadata_file,
    load_metadata_from_local_or_remote,
    save_metadata_to_local_or_remote,
)
command_line_logger = RankedLogger(__name__, rank_zero_only=True)
F = TypeVar("F", bound=Callable[..., Any])
class RestartAndLoadCheckpointCallback(Callback):
    """A callback that saves checkpoints and metadata for job restart capabilities.

    This callback is designed to provide restart functionality for Lightning training jobs,
    capturing metadata about the training environment and handling exceptions by
    initiating clean restarts.

    It saves checkpoint metadata to disk (either local or remote storage) to enable
    job resumption after failures. The callback tracks distributed training configuration,
    port usage, and restart history.

    Attributes:
        metadata_dir (str): Directory where checkpoints and metadata are saved
        metadata_path (str): Path to the metadata JSON file
        metadata (JobCheckpointMetadata): Object containing all restart metadata

    Key functionality:
    - Tracks distributed training configuration (world size, node rank, etc.)
    - Logs exceptions when training failures occur
    - Cleans up resources (distributed processes, CUDA memory) during failures
    - Maintains a history of job restarts with exception information
    - Supports both local and remote metadata storage

    Example:
        ```python
        trainer = Trainer(
            callbacks=[RestartAndLoadCheckpointCallback(metadata_dir="./checkpoints")]
        ```
    """

    def __init__(self, metadata_dir: Union[str, None] = None):
        """
        Initialize the RestartAndLoadCheckpointCallback.

        Args:
            metadata_dir (str): Directory where checkpoints and metadata will be saved.
        """
        super().__init__()
        if metadata_dir is None:
            metadata_dir = os.environ.get("METADATA_DIR", "")
        else:
            if metadata_dir != os.environ.get("METADATA_DIR", ""):
                os.environ["METADATA_DIR"] = str(metadata_dir)
        self.metadata_dir = metadata_dir
        os.makedirs(self.metadata_dir, exist_ok=True)

        self.metadata_path = os.path.join(self.metadata_dir, "restart_metadata.json")
        self._load_metadata()
        self._save_metadata()

    def _load_metadata(self) -> None:
        # Load metadata from disk, if it exists, then save it to the metadata attribute
        self.metadata: JobCheckpointMetadata = load_metadata_from_local_or_remote(
            self.metadata_path
        )

    @rank_zero_only
    def _save_metadata(self) -> None:
        """Save metadata to disk (from rank 0 only)."""
        save_metadata_to_local_or_remote(self.metadata, self.metadata_path)

    @rank_zero_only
    def on_train_start(self, trainer, pl_module) -> None:
        """Record distributed training configuration on training start."""
        self.metadata.world_size = trainer.world_size
        self.metadata.node_rank = getattr(trainer, "node_rank", 0)
        self.metadata.master_addr = os.environ.get("MASTER_ADDR", "")

        # Track used ports
        master_port = os.environ.get("MASTER_PORT", "")
        if master_port and master_port not in self.metadata.used_ports:
            self.metadata.used_ports.append(master_port)

        self._save_metadata()

    def on_exception(self, trainer, pl_module, exception) -> None:
        """Handle exceptions by saving state and initiating restart if needed."""
        command_line_logger.error(f"Exception caught: {exception}")
        command_line_logger.error(f"Stack trace: {traceback.format_exc()}")

        command_line_logger.info("Cleaning up and handling restart logic")

        self.metadata.current_run = get_attribute_from_metadata_file(
            self.metadata_path, "current_run"
        )

        self.metadata.restarts.append(
            RestartMetadata(
                time=datetime.now().isoformat(),
                exception=str(exception),
                run_number=self.metadata.current_run,
            ).to_dict()
        )
        self.metadata.current_run += 1
        command_line_logger.info(f"Restarting job. Current metadata: {self.metadata}")
        self._save_metadata()

        # Clean up resources
        self._cleanup_resources(trainer, exception)

    def _cleanup_resources(self, trainer: Trainer, exception: Exception) -> None:
        clean_up_resources(trainer, exception)

        command_line_logger.info(f"Resources cleaned up.")
        os._exit(1)
class BaseJobLauncher:
    """Base class for job launchers with retry functionality.

    This class provides a foundation for executing jobs with automatic retry capabilities.
    It handles setup of metadata directories, command preparation, and job execution with
    configurable retry logic.

    Attributes:
        cfg (DictConfig): Configuration dictionary for the job.
        logger (RankedLogger): Logger instance for output messages.
        max_retries (int): Maximum number of retry attempts (default: 3).
        retry_delay (int): Delay in seconds between retry attempts (default: 5).
        run_count (int): Counter for the number of execution attempts.
        metadata_dir (str): Directory path for storing metadata.
        process (subprocess.Popen): Process object for the running job.

    Methods:
        setup_metadata_dir(): Configures the metadata directory for the job.
        prepare_command(): Prepares the command to be executed.
        execute_job(): Executes the job with retry logic.
        _clean_process(): Cleans up process and its children.
        run_single_attempt(cmd): Runs a single attempt of the job.
        cleanup(): Cleans up resources and exits.
        launch(function_to_run): Abstract method to be implemented by subclasses.
    """

    def __init__(self, cfg: DictConfig, max_retries: int = 3, retry_delay: int = 5):
        self.cfg = cfg
        self.logger = RankedLogger(__name__, rank_zero_only=True)
        self.max_retries = cfg.get("max_retries", max_retries)
        self.retry_delay = cfg.get("retry_delay", retry_delay)
        self.run_count = 0
        self.metadata_dir = None
        self.process = None

    def setup_metadata_dir(self):
        """We look for the path first on the callback, then on the paths folder. If none is available,
        we create a temporary directory."""
        if (
            self.cfg.get("callbacks")
            and self.cfg.callbacks.get("restart_job", None)
            and self.cfg.callbacks.restart_job.metadata_dir
        ):
            self.metadata_dir = self.cfg.callbacks.restart_job.metadata_dir
        elif self.cfg.paths.get("metadata_dir"):
            self.metadata_dir = self.cfg.paths.metadata_dir
        else:
            self.logger.info("Creating temporary metadata dir.")
            self.metadata_dir = tempfile.mkdtemp(prefix=f"job_launcher_{os.getpid()}_")

        os.environ["METADATA_DIR"] = self.metadata_dir
        self.cfg.paths.metadata_dir = self.metadata_dir
        self.metadata_path = os.path.join(self.metadata_dir, "restart_metadata.json")
        self.logger.info(f"Using metadata dir: {self.metadata_dir}")

    def prepare_command(self):
        """Prepare the command to execute."""
        original_argv = sys.argv
        extra_commands = _copy_time_dependent_args(
            self.cfg, original_args=original_argv
        ) + [
            "++extras.print_config=False",
        ]

        cmd = [sys.executable] + original_argv + ["++should_skip_retry=True"]

        cmd += extra_commands

        return cmd

    def update_command_with_retry_count(self, cmd):
        """Update the command with the retry count."""
        for i in range(len(cmd)):
            if f"retry-{self.run_count-1}" in cmd[i]:
                cmd[i] = cmd[i].replace(
                    f"retry-{self.run_count-1}", f"retry-{self.run_count}"
                )

        return cmd

    def execute_job(self) -> bool:
        """Execute the job with retries."""
        self.setup_metadata_dir()
        cmd = self.prepare_command()

        while self.run_count <= self.max_retries:
            cmd = self.update_command_with_retry_count(cmd)
            self.logger.info(
                f"Starting job (attempt {self.run_count + 1}/{self.max_retries + 1}): {' '.join(cmd)}"
            )

            success = self.run_single_attempt(cmd)

            if success:
                self.logger.info("Job finished successfully.")
                return True

            self.run_count += 1
            metadata_run = get_attribute_from_metadata_file(
                self.metadata_path, "current_run"
            )
            if metadata_run != self.run_count:
                self.logger.error(
                    f"Metadata run count {metadata_run} does not match current run count {self.run_count}."
                    f"This can happen if the exception does not trigger the callback on_exception. Cleaning up and"
                    f"updating metadata before retrying."
                )

                clean_up_resources()
                metadata = load_metadata_from_local_or_remote(self.metadata_path)
                metadata.current_run = self.run_count
                save_metadata_to_local_or_remote(metadata, self.metadata_path)
            if self.run_count <= self.max_retries:
                self.logger.info(
                    f"Retrying in {self.retry_delay} seconds. Attempt {self.run_count + 1}/{self.max_retries + 1}"
                )
                time.sleep(self.retry_delay)
            else:
                self.logger.error(f"Job failed after {self.max_retries} retries.")
                return False

        return False

    def _clean_process(self):
        """Clean up the process and its children."""
        if self.process and self.process.poll() is None:
            self.process.send_signal(signal.SIGKILL)

    def run_single_attempt(self, cmd) -> bool:
        """Run a single attempt of the job."""
        self.process = subprocess.Popen(cmd, env=os.environ)

        try:
            while _is_process_running(self.process):
                time.sleep(5)

            if self.process.returncode == 0:
                return True

            self._clean_process()
            self.logger.error(f"Job failed with return code {self.process.returncode}")
            self.process = None
            return False

        except Exception as e:
            self.logger.error(f"Error running job: {e} {traceback.format_exc()}")
            self._clean_process()
            return False

    def cleanup(self):
        """Clean up resources."""
        return_code = self.process.returncode if self.process else None
        self._clean_process()
        sys.exit(return_code if return_code is not None else 1)

    def launch(self, function_to_run: callable) -> bool:
        raise NotImplementedError(
            "Method not implemented, should be implemented in child classes."
        )
class LocalJobLauncher(BaseJobLauncher):
    """Job Launcher for local execution with retry capability.

    This class handles the execution of jobs locally with support for automatic retries
    in case of failure. It can be configured to skip retry logic, particularly for
    multi-node training scenarios where retries are not supported.

    Parameters
    ----------
    cfg : DictConfig
        Configuration dictionary containing job parameters.
    max_retries : int, optional
        Maximum number of retry attempts if a job fails, by default 3.
    retry_delay : int, optional
        Delay in seconds between retry attempts, by default 5.

    Attributes
    ----------
    should_skip_retry : bool
        Flag indicating whether retry logic should be skipped. Automatically set to True
        for multi-node training configurations.

    Notes
    -----
    For multi-node training (trainer.num_nodes > 1), retry logic is automatically
    disabled by setting should_skip_retry to True.
    """

    def __init__(self, cfg: DictConfig, max_retries: int = 3, retry_delay: int = 5):
        super().__init__(cfg, max_retries, retry_delay)
        self.should_skip_retry = cfg.get("should_skip_retry", False)
        if (
            not self.should_skip_retry
            and cfg.get("trainer")
            and cfg.trainer.num_nodes > 1
        ):
            self.logger.warning(
                "Retry logic is not supported for multi-node training, setting should_skip_retry to True."
            )
            self.should_skip_retry = True

        if self.should_skip_retry:
            self.max_retries = 0

    def launch(self, function_to_run: callable) -> bool:
        """Main entry point for launching a job."""
        if self.should_skip_retry:
            self.logger.info("should_skip_retry set to True. Skipping retry logic.")
            return function_to_run(cfg=self.cfg)
        try:
            return self.execute_job()
        except Exception as e:
            self.logger.error(
                f"Error launching job. Exception {e} {traceback.format_exc()}"
            )
            self.cleanup()
            return False
        except KeyboardInterrupt:
            self.logger.info("Keyboard interrupt caught. Exiting.")
            self._clean_process()
            return False
        finally:
            self.cleanup()
class VertexAIJobLauncher(BaseJobLauncher):
    """
    A job launcher for Vertex AI that extends BaseJobLauncher to handle job retries on Vertex AI platform.

    This launcher provides configurable retry logic for both the job launch itself and for runs within the job.
    It offers different retry strategies for single-node and multi-node training jobs.

    Parameters:
    ----------
    cfg : DictConfig
        The configuration dictionary containing job parameters and Vertex AI settings.
    max_retries : int, default=3
        Maximum number of retry attempts for the job.
    retry_delay : int, default=5
        Delay in seconds between retry attempts.

    Attributes:
    ----------
    should_skip_job_launch_retry : bool
        Controls whether to retry the job launch itself. When True, only in-job retries will occur.
        When False, the job launching mechanism will be retried on failure.

    Notes:
    -----
    Recommended configurations:
    - Single node jobs: should_skip_retry=False, should_skip_job_launch_retry=True
    - Multi node jobs: should_skip_retry=True, should_skip_job_launch_retry=False

    For increased resilience, increase the max_retries value.
    For maximum resilience (but potentially infinite retries), set both skip flags to False.
    """

    def __init__(self, cfg: DictConfig, max_retries: int = 3, retry_delay: int = 5):
        """
        Initialize the restart job handler.

        This class manages retry behavior for job execution, offering different strategies for single-node
        and multi-node training scenarios.

        Args:
            cfg (DictConfig): Configuration dictionary containing settings for the job.
            max_retries (int, optional): Maximum number of retry attempts. Defaults to 3.
            retry_delay (int, optional): Delay in seconds between retry attempts. Defaults to 5.

        Notes:
            Default behavior is to retry runs but not the job launch. The VAI job launcher will submit
            a run that will operate like a local run during the VAI job.

            For multinode training, the local retry logic is not supported since multinode process
            handling is complex, so those flags should be configured differently.

            Recommendations:
            - Single node jobs: should_skip_retry=False, should_skip_job_launch_retry=True
            - Multi node jobs: should_skip_retry=True, should_skip_job_launch_retry=False

            To make jobs more resilient, increase the number of max_retries.
            To allow almost unlimited retries, set both skip flags to False.
        """
        super().__init__(cfg, max_retries, retry_delay)

        self.should_skip_job_launch_retry = cfg.get(
            "should_skip_job_launch_retry", True
        )
        if self.should_skip_job_launch_retry:
            self.max_retries = 0

    def prepare_command(self):
        original_argv = sys.argv
        vai_args = [args for args in original_argv if "vai_args" in args][0]
        vai_args = vai_args.replace("++vai_args=", "").strip("'")
        vai_args = vai_args.split(" ")

        extra_commands = _copy_time_dependent_args(self.cfg, vai_args)

        # We pass the parameters to the VAI run through the vai_args field. We update it
        # with the extra commands we want to pass to the subprocess.
        for i in range(len(original_argv)):
            if "vai_args" in original_argv[i]:
                original_argv[
                    i
                ] = f"{original_argv[i][:-1]} {' '.join(extra_commands)}'"

        cmd = [sys.executable] + original_argv + ["++should_skip_job_launch_retry=True"]

        if not self.should_skip_job_launch_retry:
            cmd += [
                f"++vertexai.custom_job_config.display_name={self.cfg.vertexai.custom_job_config.display_name}-retry-{self.run_count}"
            ]
        return cmd

    def launch(self, function_to_run: callable) -> bool:
        if self.should_skip_job_launch_retry:
            self.logger.info(
                "should_skip_job_launch_retry set to True. Skipping job launch retry logic."
            )

            return function_to_run(cfg=self.cfg)
        try:
            return self.execute_job()
        except Exception as e:
            self.logger.error(
                f"Error launching job. Exception {e} {traceback.format_exc()}"
            )
            return False
        except KeyboardInterrupt:
            self.logger.info(
                "Keyboard interrupt caught. This won't kill the job, for that you should use the VertexAI UI. Killing the job here only kills the retry logic."
            )
            return False
        finally:
            self.cleanup()